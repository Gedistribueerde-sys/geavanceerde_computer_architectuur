\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[colorlinks, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
\usepackage{geometry}
\geometry{tmargin=3cm, bmargin=2.2cm, lmargin=2.2cm, rmargin=2cm}
\usepackage{todonotes} %Used for the figure placeholders
\usepackage{ifthen}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor} % good practice to load this explicitly before using \color
\usepackage{url} % of \usepackage{hyperref}

% Your name and student number must be filled in on the title page found in
% titlepage.tex.
\lstdefinelanguage{CUDA}{
  language=C++,
  morekeywords={
    __global__, __device__, __host__, __shared__, __constant__,
    __syncthreads, __threadfence_block, __threadfence, __threadfence_system,
    threadIdx, blockIdx, blockDim, gridDim
  },
  sensitive=true
}

% Algemene stijl
\lstset{
  language=CUDA,            % Use custom defined language
    basicstyle=\ttfamily\footnotesize,      % Monospaced font, small
    keywordstyle=[1]\color{blue}\bfseries,  % Mnemonics in blue and bold
    keywordstyle=[2]\color{purple},         % Registers in purple
    keywordstyle=[3]\color{brown}\bfseries, % Labels in brown and bold
    keywordstyle=[4]\color{teal}\bfseries,  % Constants and preprocessor in teal
    commentstyle=\color{gray},              % Comments in gray
    stringstyle=\color{red},                % Strings in red
    numbers=left,                           % Line numbers on the left
    numberstyle=\tiny\color{gray},          % Line number style
    stepnumber=1,                           % Line numbering step
    numbersep=5pt,                          % Space between line numbers and code
    breaklines=true,                        % Line breaking
    backgroundcolor=\color{gray!20},        % Light gray background
    frame=single,                           % Single frame around the code block
    captionpos=b,                           % Caption position at the bottom
    escapeinside={\%*}{*)},                 % Escape to LaTeX with %*...*)
}

\begin{document}
\newboolean{anonymize}
% Uncomment to create an anonymized version of your report
%\setboolean{anonymize}{true}

\input{titlepage}

\tableofcontents
\newpage
\section{Introduction}
This report talks about the implementation of a point cloud to voxel grid filter. The reason for implementing this filter is to reduce the number of points in a point cloud, while preserving the overall structure and appearance of the original data. Doing this on a CPU takes a long time for large point clouds, so the filter is implemented on a GPU using CUDA to take advantage of the parallel processing capabilities of modern graphics hardware.
\section{Implementation}
In this chapter, the practical implementation details of the point cloud to voxel grid filter are presented. To leverage the massive parallelism of modern hardware, the algorithms were developed using CUDA for GPU acceleration. Two distinct parallel strategies were designed to solve the voxelization problem: a sorting-based approach using Morton encoding, and a scattering approach using a GPU-resident dynamic hash map.

\subsection{Morton and Sort Voxelizer}
The first approach utilizes the Thrust library to perform a sort-and-reduce operation. This method relies on the principle of organizing data such that points belonging to the same voxel are stored contiguously in memory.

The algorithm proceeds in the following stages:
\begin{enumerate}
    \item Quantization: First, the global bounding box of the point cloud is calculated. For every point, discrete grid coordinates are computed based on the user-defined voxel size.
    \item Encoding: The 3D grid coordinates are mapped to a 1D scalar value using Z-order curve encoding. This is achieved by bit-interleaving the integer coordinates to produce a 64-bit integer, which serves as the unique key for the voxel.
    \item Sorting: The point indices are sorted based on their generated Morton codes. This ensures that all points residing in the same spatial voxel are grouped together in the linear array.
    \item Reduction: A reduction-by-key operation is performed. This step iterates through the sorted array, identifies segments of identical Morton codes, and sums the coordinate and color data for each segment.
    \item Centroid Calculation: Finally, the accumulated sums are divided by the point count per voxel to yield the final downsampled point cloud.
\end{enumerate}

The theoretical complexity of this approach is dominated by the sorting phase. Given N input points, the time complexity is O(N log N). This method offers deterministic memory usage and stable performance regardless of the spatial distribution of the points.

\subsection{Dynamic Hash Map Voxelizer}
The second approach implements a custom open-addressing hash table directly in GPU global memory. This method is designed for maximum throughput, avoiding the global synchronization overhead required by sorting.

Crucially, this implementation re-purposes the Morton encoding scheme employed in the previous method. Instead of using the code for sorting, the 64-bit Morton integer serves as a compact, unique hash key. This allows a complex three-dimensional coordinate triplet to be treated as a single primitive type, enabling efficient atomic operations within the hardware registers.

The architecture operates through a high-throughput scatter-and-accumulate pipeline:
\begin{enumerate}
    \item Initialization: A hash table is allocated in VRAM. To mitigate the risk of collisions inherent to hashing, the capacity is set significantly larger than the number of input points (typically a factor of 2.0 to 4.0).
    \item Insertion and Accumulation: A custom kernel processes points in parallel. For each point, the grid coordinates are converted into a Morton code. This code is then hashed to find an initial slot in the table. The algorithm uses a linear probing strategy combined with lock-free atomic primitives:
    \begin{itemize}
        \item A thread attempts to claim a bucket using an atomic Compare-and-Swap (\texttt{atomicCAS}).
        \item If the bucket is empty, the thread successfully claims the voxel ownership.
        \item If the bucket holds the same Morton key, the thread accumulates its point data into the existing voxel using \texttt{atomicAdd}.
        \item If the bucket is occupied by a different key (a collision), the thread probes the subsequent memory slot until a valid location is found.
    \end{itemize}
    \item Compaction: Because the hashing process leaves gaps in the table, a final collection pass scans the memory to extract only the valid, populated voxels and writes them densely into the output buffer.
\end{enumerate}

Ideally, this approach achieves O(N) complexity, processing each point in constant time. By treating the Morton code as a hash key, the algorithm creates a "race" where threads compete to update voxel data simultaneously. While this is generally faster than sorting, performance is sensitive to the load factor. As the table fills, threads must probe further to find open slots, which can degrade performance. However, for massive point clouds with sufficient memory available, this method typically yields the highest processing speeds.
\section{Results and Analysis}

\subsection{General Observations}
The voxelizers are compared among five voxel sizes (0.25, 0.5, 0.75, 1.0, 1.25). For each voxel size, eight block sizes were tested (1, 2, 4, 8, 16, 32, 64, 256, 512, 1024 threads per block). The hash-table voxelizer was evaluated with three different capacity factors (2, 3, and 4 times the number of input points). The performance metrics recorded include total execution time and a breakdown of time spent in key phases of each algorithm.
Every configuration was run 100 times to obtain an average execution time, minimizing the impact of transient system load variations.

\subsection{Performance Analysis per Method}

\subsubsection{Morton-Code Voxelizer}
The Morton-based approach demonstrates consistent and predictable performance. Key findings include:

\begin{itemize}
    \item \textbf{Optimal block size:} 4--256 threads per block.
    \item \textbf{Total execution time:} Ranging from 25.18\,ms (voxel size 1.25, block size 64) to 40.46\,ms (voxel size 0.25, block size 1).
    \item \textbf{Primary bottleneck:} The sorting stage dominates runtime, requiring approximately 0.90--1.02\,ms, nearly constant across all tests.
\end{itemize}

Very small block sizes (1--4 threads) significantly slow down Morton-code generation (0.27--1.07\,ms), while larger block sizes stabilize this step to around 0.04\,ms.

\subsubsection{Hash-Table Voxelizer}
The hash-based method demonstrates greater variability, largely influenced by the selected capacity factor of the hash table.

\paragraph{Capacity Factor 2 (highest efficiency)}
\begin{itemize}
    \item \textbf{Best overall performance:} 25.67\,ms (voxel size 1.0, block size 32).
    \item \textbf{Optimal block sizes:} 8--32 threads per block.
    \item \textbf{Advantages:} Lowest memory overhead and fastest device-to-host transfer times.
\end{itemize}

\paragraph{Capacity Factor 3 (balanced)}
\begin{itemize}
    \item Execution times are \textbf{2--4\,ms slower} compared to capacity factor 2.
    \item Reduced collisions during accumulation, at the cost of increased memory usage.
\end{itemize}

\paragraph{Capacity Factor 4 (highest overhead)}
\begin{itemize}
    \item \textbf{5--10\,ms slower} than capacity factor 2.
    \item Initialization and memory operations increase disproportionately.
    \item Device-to-host + cleanup time rises up to 5.26\,ms (compared to 2.71\,ms for CF=2).
\end{itemize}

\subsection{Impact of Voxel Size}
Smaller voxel sizes generate a significantly larger number of unique voxel entries. This favors the hash-table method with a low capacity factor, as the Morton approach becomes less efficient when spatial data is highly fragmented.

For larger voxels (e.g., voxel size 1.25), the Morton method benefits from predictable spatial coherence and memory access patterns, making it more efficient than the hash-based approach.

\subsection{Impact of Block Size}
\begin{itemize}
    \item \textbf{Block sizes 1--4:} Strong performance penalties due to insufficient warp utilization.
    \item \textbf{Block sizes 8--256:} Optimal performance range with minimal overhead.
    \item \textbf{Block sizes 512--1024:} No significant improvements; potentially limited by register pressure.
\end{itemize}

\subsection{Timing Breakdown}

\subsubsection{Morton Method}
Dominant phases:
\begin{enumerate}
    \item \textbf{Sorting:} 0.90--1.02\,ms (consistent across tests)
    \item \textbf{Morton code computation:} 0.04--1.07\,ms (strongly dependent on block size)
    \item \textbf{Point accumulation:} 0.24--1.08\,ms
\end{enumerate}

\subsubsection{Hash Method}
Dominant phases:
\begin{enumerate}
    \item \textbf{Device-to-host transfer + cleanup:} 2.71--5.26\,ms (increases with capacity factor)
    \item \textbf{Initialization:} 0.33--3.71\,ms (proportional to hash-table size)
    \item \textbf{Populate phase:} 0.31--2.46\,ms (collision-sensitive)
\end{enumerate}

\subsection{Conclusions}

\subsubsection{Optimal Configurations}
The analysis indicates that different voxel sizes benefit from different GPU strategies:

\begin{itemize}
    \item \textbf{Small voxel sizes (0.25--0.5):} Hash-based voxelizer with capacity factor 2 and a block size of 16--32 threads.
    \item \textbf{Medium voxel sizes (0.75--1.0):} Hash-based voxelizer with capacity factor 2 and a block size of 32 threads.
    \item \textbf{Large voxel sizes (1.25 and above):} Morton-code voxelizer with a block size between 64 and 256 threads.
\end{itemize}

\subsubsection{Explanation}
These optimal configurations follow from the observed behaviour of both voxelization methods:

\begin{enumerate}
    \item For small voxel sizes, the hash method with capacity factor 2 achieves the highest throughput due to low memory overhead and efficient device-to-host transfers.
    \item For larger voxel sizes, the Morton-based approach becomes more efficient thanks to spatial coherence and predictable memory access patterns.
    \item Block sizes between 16 and 64 threads provide the most balanced performance in terms of occupancy and register usage.
\end{enumerate}

\subsubsection{Performance Limitations}
Despite their strengths, both methods exhibit certain limitations:

\begin{itemize}
    \item The hash-based method is primarily constrained by memory bandwidth, especially during device-to-host transfers.
    \item The Morton-based method is strongly dominated by the sorting stage, which forms its main computational bottleneck.
    \item For both approaches, increasing block sizes beyond 256 threads yields minimal benefits due to hardware saturation.
\end{itemize}


\input{figures}
\section{Possible Improvements}
The main problem is that both implementations work with a fixed voxel size. This means that if the point cloud is very large but sparse, a lot of memory is wasted on empty space. A possible improvement would be to implement an octree structure, which would allow for variable voxel sizes depending on the density of points in a given area.

\input{bibliography}
\end{document}
