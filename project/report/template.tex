\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[colorlinks, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
\usepackage{geometry}
\geometry{tmargin=3cm, bmargin=2.2cm, lmargin=2.2cm, rmargin=2cm}
\usepackage{todonotes} %Used for the figure placeholders
\usepackage{ifthen}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor} % good practice to load this explicitly before using \color
\usepackage{url} % of \usepackage{hyperref}

% Your name and student number must be filled in on the title page found in
% titlepage.tex.
\lstdefinelanguage{CUDA}{
  language=C++,
  morekeywords={
    __global__, __device__, __host__, __shared__, __constant__,
    __syncthreads, __threadfence_block, __threadfence, __threadfence_system,
    threadIdx, blockIdx, blockDim, gridDim
  },
  sensitive=true
}

% Algemene stijl
\lstset{
  language=CUDA,            % Use custom defined language
    basicstyle=\ttfamily\footnotesize,      % Monospaced font, small
    keywordstyle=[1]\color{blue}\bfseries,  % Mnemonics in blue and bold
    keywordstyle=[2]\color{purple},         % Registers in purple
    keywordstyle=[3]\color{brown}\bfseries, % Labels in brown and bold
    keywordstyle=[4]\color{teal}\bfseries,  % Constants and preprocessor in teal
    commentstyle=\color{gray},              % Comments in gray
    stringstyle=\color{red},                % Strings in red
    numbers=left,                           % Line numbers on the left
    numberstyle=\tiny\color{gray},          % Line number style
    stepnumber=1,                           % Line numbering step
    numbersep=5pt,                          % Space between line numbers and code
    breaklines=true,                        % Line breaking
    backgroundcolor=\color{gray!20},        % Light gray background
    frame=single,                           % Single frame around the code block
    captionpos=b,                           % Caption position at the bottom
    escapeinside={\%*}{*)},                 % Escape to LaTeX with %*...*)
}

\begin{document}
\newboolean{anonymize}
% Uncomment to create an anonymized version of your report
%\setboolean{anonymize}{true}

\input{titlepage}

\tableofcontents
\newpage
\section{Introduction}

This report talks about the implementation of a point cloud to voxel grid filter. The reason for implementing this filter is to reduce the number of points in a point cloud, while preserving the overall structure and appearance of the original data. Doing this on a CPU takes a long time for large point clouds, so the filter is implemented on a GPU using CUDA to take advantage of the parallel processing capabilities of modern graphics hardware.
\input{theory}

\section {Voxel size}
The voxel size is an important parameter in the point cloud to voxel filter. The bigger the voxel size ,the more points will be grouped together into a single voxel, resulting in a coarser representation of the original point cloud. Conversely, a smaller voxel size will result in a finer representation, with more voxels and fewer points per voxel.

\input{implementation}
\section{Results and Analysis}
\subsection{Timing remarks and methodology}
All the following results were obtained on relatively old hardware: a NVIDIA Geforce GTX 1080ti with 11GB of GDDR5X memory, paired with an Intel Core i7-8700K CPU @ 4.8 GHz and 32GB of DDR4 RAM and the pc runs ubuntu 24.04.3 LTS. The point cloud used for testing contains 648433 points, representing a dense scan of an urban environment.
\subsection{General Observations}
The voxelizers are compared among five voxel sizes (0.25, 0.5, 0.75, 1.0, 1.25). For each voxel size, eight block sizes were tested (1, 2, 4, 8, 16, 32, 64, 256, 512, 1024 threads per block). The hash-table voxelizer was evaluated with three different capacity factors (2, 3, and 4 times the number of input points). The performance metrics recorded include total execution time and a breakdown of time spent in key phases of each algorithm.
Every configuration was run 100 times to obtain an average execution time, minimizing the impact of transient system load variations.

\subsection{Performance Analysis per Method}

\subsubsection{Morton-Code Voxelizer}
The Morton-based approach demonstrates consistent and predictable performance. Key findings include:

\begin{itemize}
    \item \textbf{Optimal block size:} 4--256 threads per block.
    \item \textbf{Total execution time:} Ranging from 25.18\,ms (voxel size 1.25, block size 64) to 40.46\,ms (voxel size 0.25, block size 1).
    \item \textbf{Primary bottleneck:} The sorting stage dominates runtime, requiring approximately 0.90--1.02\,ms, nearly constant across all tests.
\end{itemize}

Very small block sizes (1--4 threads) significantly slow down Morton-code generation (0.27--1.07\,ms), while larger block sizes stabilize this step to around 0.04\,ms.

\subsubsection{Hash-Table Voxelizer}
The hash-based method demonstrates greater variability, largely influenced by the selected capacity factor of the hash table.

\paragraph{Capacity Factor 2 (highest efficiency)}
\begin{itemize}
    \item \textbf{Best overall performance:} 25.67\,ms (voxel size 1.0, block size 32).
    \item \textbf{Optimal block sizes:} 8--32 threads per block.
    \item \textbf{Advantages:} Lowest memory overhead and fastest device-to-host transfer times.
\end{itemize}

\paragraph{Capacity Factor 3 (balanced)}
\begin{itemize}
    \item Execution times are \textbf{2--4\,ms slower} compared to capacity factor 2.
    \item Reduced collisions during accumulation, at the cost of increased memory usage.
\end{itemize}

\paragraph{Capacity Factor 4 (highest overhead)}
\begin{itemize}
    \item \textbf{5--10\,ms slower} than capacity factor 2.
    \item Initialization and memory operations increase disproportionately.
    \item Device-to-host + cleanup time rises up to 5.26\,ms (compared to 2.71\,ms for CF=2).
\end{itemize}

\subsection{Impact of Voxel Size}
Smaller voxel sizes generate a significantly larger number of unique voxel entries. This favors the hash-table method with a low capacity factor, as the Morton approach becomes less efficient when spatial data is highly fragmented.

For larger voxels (e.g., voxel size 1.25), the Morton method benefits from predictable spatial coherence and memory access patterns, making it more efficient than the hash-based approach.

\subsection{Impact of Block Size}
\begin{itemize}
    \item \textbf{Block sizes 1--4:} Strong performance penalties due to insufficient warp utilization.
    \item \textbf{Block sizes 8--256:} Optimal performance range with minimal overhead.
    \item \textbf{Block sizes 512--1024:} No significant improvements; potentially limited by register pressure.
\end{itemize}

\subsection{Timing Breakdown}

\subsubsection{Morton Method}
Dominant phases:
\begin{enumerate}
    \item \textbf{Sorting:} 0.90--1.02\,ms (consistent across tests)
    \item \textbf{Morton code computation:} 0.04--1.07\,ms (strongly dependent on block size)
    \item \textbf{Point accumulation:} 0.24--1.08\,ms
\end{enumerate}

\subsubsection{Hash Method}
Dominant phases:
\begin{enumerate}
    \item \textbf{Device-to-host transfer + cleanup:} 2.71--5.26\,ms (increases with capacity factor)
    \item \textbf{Initialization:} 0.33--3.71\,ms (proportional to hash-table size)
    \item \textbf{Populate phase:} 0.31--2.46\,ms (collision-sensitive)
\end{enumerate}

\subsection{Conclusions}

\subsubsection{Optimal Configurations}
The analysis indicates that different voxel sizes benefit from different GPU strategies:

\begin{itemize}
    \item \textbf{Small voxel sizes (0.25--0.5):} Hash-based voxelizer with capacity factor 2 and a block size of 16--32 threads.
    \item \textbf{Medium voxel sizes (0.75--1.0):} Hash-based voxelizer with capacity factor 2 and a block size of 32 threads.
    \item \textbf{Large voxel sizes (1.25 and above):} Morton-code voxelizer with a block size between 64 and 256 threads.
\end{itemize}

\subsubsection{Explanation}
These optimal configurations follow from the observed behaviour of both voxelization methods:

\begin{enumerate}
    \item For small voxel sizes, the hash method with capacity factor 2 achieves the highest throughput due to low memory overhead and efficient device-to-host transfers.
    \item For larger voxel sizes, the Morton-based approach becomes more efficient thanks to spatial coherence and predictable memory access patterns.
    \item Block sizes between 16 and 64 threads provide the most balanced performance in terms of occupancy and register usage.
\end{enumerate}

\subsubsection{Performance Limitations}
Despite their strengths, both methods exhibit certain limitations:

\begin{itemize}
    \item The hash-based method is primarily constrained by memory bandwidth, especially during device-to-host transfers.
    \item The Morton-based method is strongly dominated by the sorting stage, which forms its main computational bottleneck.
    \item For both approaches, increasing block sizes beyond 256 threads yields minimal benefits due to hardware saturation.
\end{itemize}

\subsection {gpu vs cpu performance}
\textbf{TODOOOOOOOOOOOOOOOOOOOOOOOOOO}


\input{figures}
\section{Visualisation}
Everything was visualised using the python open3d library. This library allows for easy loading and displaying of point clouds. The original point cloud and the voxelized point cloud were displayed side by side for comparison.
\textbf{TODOOOO : paste figures}
\section{Possible Improvements}
The main problem is that both implementations work with a fixed voxel size. This means that if the point cloud is very large but sparse, a lot of memory is wasted on empty space. A possible improvement would be to implement an octree structure, which would allow for variable voxel sizes depending on the density of points in a given area.

\input{bibliography}
\end{document}
