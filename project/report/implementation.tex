\section{Implementation}
In this chapter, the practical implementation details of the point cloud to voxel grid filter are presented. To leverage the massive parallelism of modern hardware, the algorithms were developed using CUDA for GPU acceleration. Two distinct parallel strategies were designed to solve the voxelization problem: a sorting-based approach using Morton encoding, and a scattering approach using a GPU-resident dynamic hash map. This hashmap uses the same Morton encoding for voxel indexing, ensuring consistency between both methods.
Both voxelizers share the same Morton encoding utilities so that a 3D voxel index is always mapped to the same 64-bit key, independent of the backend. On top of that, the Morton-and-sort voxelizer introduces its own per-voxel accumulation structure and reduction functor, while the hash-based implementation uses a dedicated hash bucket structure.

\subsection{Shared Morton Encoding Utilities}

\paragraph{Bit expansion helper.}
The first shared building block is the mapping from 3D integer voxel coordinates $(i_x,i_y,i_z)$ to a single 64-bit Morton code. This is reused in \emph{both} voxelizers: in the sorting-based pipeline it is used to generate keys for \texttt{thrust::sort\_by\_key}, while in the hash-based pipeline the same 64-bit Morton code serves directly as the hash key for open addressing.

The helper function \texttt{splitBy3} expands a 21-bit integer into a 64-bit value in which each original bit is separated by two zero bits:

\begin{lstlisting}[language=CUDA,caption={Bit expansion of a 21-bit coordinate into Morton layout (shared).}]
__device__ __host__ inline uint64_t splitBy3(uint64_t v) {
    v = (v | (v << 32)) & 0x1f00000000ffffULL;
    v = (v | (v << 16)) & 0x1f0000ff0000ffULL;
    v = (v | (v <<  8)) & 0x100f00f00f00f00fULL;
    v = (v | (v <<  4)) & 0x10c30c30c30c30c3ULL;
    v = (v | (v <<  2)) & 0x1249249249249249ULL;
    return v;
}
\end{lstlisting}

\paragraph{Morton code generation.}
Using \texttt{splitBy3}, the actual Morton code is formed by interleaving the expanded bits of the three coordinates:

\begin{lstlisting}[language=CUDA,caption={Morton encoding of 3D voxel indices (shared).}]
__device__ __host__ inline uint64_t mortonEncode(uint32_t x,
                                                 uint32_t y,
                                                 uint32_t z) {
    return (splitBy3((uint64_t)x) << 2) |
           (splitBy3((uint64_t)y) << 1) |
            splitBy3((uint64_t)z);
}
\end{lstlisting}

Both functions are marked \texttt{\_\_device\_\_ \_\_host\_\_ inline}, so they can be called from host-side test code as well as from all CUDA kernels in both voxelization backends.

\subsection{Morton and Sort Voxelizer}

To implement the Morton-and-sort voxelizer efficiently on the GPU, the pipeline is decomposed into: (i) Morton code generation via the shared utilities, (ii) per-voxel accumulation of point attributes, and (iii) CUDA kernels that bridge raw device arrays with Thrust's sort-and-reduce primitives. The accumulation structures and reduction functor described below are specific to this implementation and are \emph{not} used by the hash-based voxelizer.

\paragraph{Per-voxel accumulation structure.}
To compute voxel centroids and average colors in a single pass after sorting, we define a compact accumulation structure \texttt{PointAccum}. It stores the running sums of coordinates and RGB components, as well as the number of points that contributed to the voxel:

\begin{lstlisting}[language=CUDA,caption={Accumulation structure for voxel-wise reduction (Morton-and-sort only).}]
struct PointAccum {
    float    sumX, sumY, sumZ;
    uint32_t sumR, sumG, sumB;
    uint32_t count;
    
    __device__ __host__ PointAccum()
        : sumX(0), sumY(0), sumZ(0),
          sumR(0), sumG(0), sumB(0),
          count(0) {}

    __device__ __host__ PointAccum(float x, float y, float z,
                                   uint8_t r, uint8_t g, uint8_t b)
        : sumX(x), sumY(y), sumZ(z),
          sumR(r), sumG(g), sumB(b),
          count(1) {}
};
\end{lstlisting}

This structure is used as the value type in the subsequent reduction-by-key stage and is therefore only required by the sorting-based voxelizer.

\paragraph{Binary reduction functor.}
To use \texttt{PointAccum} with \texttt{thrust::reduce\_by\_key}, we define a binary functor \texttt{PointAccumOp}. It describes how two partial voxel accumulators are merged:

\begin{lstlisting}[language=CUDA,caption={Binary operator for reducing \texttt{PointAccum} values (Morton-and-sort only).}]
struct PointAccumOp {
    __device__ __host__
    PointAccum operator()(const PointAccum& a,
                          const PointAccum& b) const {
        PointAccum result;
        result.sumX  = a.sumX  + b.sumX;
        result.sumY  = a.sumY  + b.sumY;
        result.sumZ  = a.sumZ  + b.sumZ;
        result.sumR  = a.sumR  + b.sumR;
        result.sumG  = a.sumG  + b.sumG;
        result.sumB  = a.sumB  + b.sumB;
        result.count = a.count + b.count;
        return result;
    }
};
\end{lstlisting}

This functor is specific to the reduction-based pipeline and is not needed by the dynamic hash map implementation.

\paragraph{Morton code computation kernel.}
The first CUDA kernel in this voxelizer takes the input point positions and converts them into Morton codes using the shared \texttt{mortonEncode} helper. Each thread processes exactly one point:

\begin{lstlisting}[language=CUDA,caption={Kernel computing Morton codes for each point (Morton-and-sort).}]
__global__ void computeMortonCodesKernel(
    const float* x,
    const float* y,
    const float* z,
    uint64_t* mortonCodes,
    float minX, float minY, float minZ,
    float invVoxelSize,
    size_t numPoints)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numPoints) return;

    // Compute voxel indices (offset by min to ensure non-negative indices)
    uint32_t ix = (uint32_t)floorf((x[idx] - minX) * invVoxelSize);
    uint32_t iy = (uint32_t)floorf((y[idx] - minY) * invVoxelSize);
    uint32_t iz = (uint32_t)floorf((z[idx] - minZ) * invVoxelSize);

    mortonCodes[idx] = mortonEncode(ix, iy, iz);
}
\end{lstlisting}

The kernel assumes a structure-of-arrays (SoA) layout (\texttt{x}, \texttt{y}, \texttt{z} in separate buffers) for coalesced memory access. The inverse voxel size is precomputed on the host to avoid divisions in device code.

\paragraph{Accumulator creation kernel.}
After sorting the points by their Morton codes, a \texttt{PointAccum} object is created for each point in sorted order. Rather than physically reordering all input arrays, a separate index array encodes the permutation induced by the sort:

\begin{lstlisting}[language=CUDA,caption={Kernel creating \texttt{PointAccum} objects after sorting (Morton-and-sort).}]
__global__ void createPointAccumKernel(
    const float*  x,
    const float*  y,
    const float*  z,
    const uint8_t* r,
    const uint8_t* g,
    const uint8_t* b,
    const uint32_t* indices,
    PointAccum* accums,
    size_t numPoints)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numPoints) return;

    uint32_t origIdx = indices[idx];
    accums[idx] = PointAccum(
        x[origIdx], y[origIdx], z[origIdx],
        r[origIdx], g[origIdx], b[origIdx]
    );
}
\end{lstlisting}

\paragraph{Integration in the outer loop.}
The outer host function for the Morton-and-sort voxelizer (not shown in full) orchestrates the following steps:

\begin{enumerate}
    \item \textbf{Preprocessing:} Compute the global bounding box and \texttt{invVoxelSize} on the CPU.
    \item \textbf{Device setup:} Upload positions and colors to device vectors; initialize an index array with \texttt{thrust::sequence}.
    \item \textbf{Morton computation:} Launch \texttt{computeMortonCodesKernel} to fill the Morton code buffer using the shared \texttt{mortonEncode}.
    \item \textbf{Sorting:} Call \texttt{thrust::sort\_by\_key} on the pair \texttt{(d\_mortonCodes, d\_indices)}.
    \item \textbf{Accumulator creation:} Launch \texttt{createPointAccumKernel} to build a \texttt{PointAccum} per sorted point.
    \item \textbf{Reduction by voxel:} Invoke \texttt{thrust::reduce\_by\_key} with \texttt{PointAccum} values and \texttt{PointAccumOp} to obtain one accumulator per occupied voxel.
    \item \textbf{Final averaging:} Copy the reduced accumulators back to the host and divide sums by \texttt{count} to obtain centroid and color per voxel.
\end{enumerate}

\subsection{Dynamic Hash Map Voxelizer}

The second approach implements a custom open-addressing hash table directly in GPU global memory. This method is designed for maximum throughput, avoiding the global synchronization and $O(N \log N)$ complexity of the sorting-based pipeline. Instead of ordering points, it scatters them into a large hash table and accumulates voxel statistics in-place using atomics.

This implementation \emph{reuses} only the shared Morton encoding helpers (\texttt{splitBy3} and \texttt{mortonEncode}). The per-voxel accumulation is performed directly inside each hash bucket, so there is no need for \texttt{PointAccum} or \texttt{PointAccumOp}.

\paragraph{Hash bucket layout.}
Each entry in the GPU hash table is represented by a tightly packed \texttt{HashBucket} structure:

\begin{lstlisting}[language=CUDA,caption={Aligned hash bucket structure and empty key sentinel (hash voxelizer).}]
struct __align__(16) HashBucket {
    unsigned long long key;  // 64-bit Morton code
    float sumX, sumY, sumZ;
    uint32_t sumR, sumG, sumB;
    uint32_t count;
};

#define EMPTY_KEY 0xFFFFFFFFFFFFFFFFULL
\end{lstlisting}

The \texttt{\_\_align\_\_(16)} qualifier improves memory coalescing, and \texttt{EMPTY\_KEY} serves as a sentinel to identify unused slots. The 64-bit \texttt{key} field stores the Morton code computed by the shared \texttt{mortonEncode} function.

\paragraph{Hash table initialization.}
Before insertion, the hash table is cleared by setting all keys to \texttt{EMPTY\_KEY} and zeroing the accumulators:

\begin{lstlisting}[language=CUDA,caption={Kernel initializing the GPU hash table (hash voxelizer).}]
__global__ void initHashMapKernel(HashBucket* table, size_t capacity) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < capacity) {
        table[idx].key   = EMPTY_KEY;
        table[idx].count = 0;

        table[idx].sumX = 0.0f;
        table[idx].sumY = 0.0f;
        table[idx].sumZ = 0.0f;
        table[idx].sumR = 0;
        table[idx].sumG = 0;
        table[idx].sumB = 0;
    }
}
\end{lstlisting}

\paragraph{Hash-Based Voxel Accumulation using Atomic Open Addressing}
The core function of the hash-based voxelizer is the $\texttt{populateHashMapKernel}$ CUDA kernel, which implements a scatter-and-accumulate pattern using a 64-bit Morton code derived from $3D$ integer coordinates $(i_x, i_y, i_z)$ via $\texttt{mortonEncode}$ as the key. Each thread calculates an initial hash slot $H = \texttt{mortonCode} \pmod{\texttt{capacity}}$ and uses linear probing to find the correct $\texttt{HashBucket}$. Concurrency is managed by atomically claiming an empty slot using $\texttt{atomicCAS}$ (Compare-and-Swap) with $\texttt{EMPTY\_KEY}$, or by finding a slot already containing its key. Once the correct slot is secured, the thread uses atomic additions ($\texttt{atomicAdd}$) to safely accumulate the point's properties (position sums, color sums, and $\texttt{count}$) into the shared $\texttt{HashBucket}$ structure, ensuring thread-safe data aggregation despite concurrent writes.
\begin{lstlisting}[language=CUDA,caption={Kernel inserting points into the GPU hash map (hash voxelizer).}]
__global__ void populateHashMapKernel(
    const float*   x, const float*   y, const float*   z,
    const uint8_t* r, const uint8_t* g, const uint8_t* b,
    HashBucket* table,
    size_t capacity,
    size_t numPoints,
    float minX, float minY, float minZ,
    float invVoxelSize)
{
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= numPoints) return;

    // 1. Quantize position to voxel grid and compute Morton key
    uint32_t ix = (uint32_t)floorf((x[idx] - minX) * invVoxelSize);
    uint32_t iy = (uint32_t)floorf((y[idx] - minY) * invVoxelSize);
    uint32_t iz = (uint32_t)floorf((z[idx] - minZ) * invVoxelSize);

    uint64_t mortonCode = mortonEncode(ix, iy, iz);

    // 2. Initial hash slot
    size_t hashIdx = mortonCode % capacity;

    // 3. Linear probing with atomic CAS and atomic adds
    for (size_t i = 0; i < capacity; ++i) {
        size_t currentSlot = (hashIdx + i) % capacity;

        unsigned long long oldKey = table[currentSlot].key;

        // Try to claim an empty slot
        if (oldKey == EMPTY_KEY) {
            unsigned long long assumed =
                atomicCAS((unsigned long long*)&table[currentSlot].key,
                          EMPTY_KEY,
                          (unsigned long long)mortonCode);
            if (assumed == EMPTY_KEY) {
                oldKey = mortonCode; // we now own this bucket
            } else {
                oldKey = assumed;    // another thread claimed it
            }
        }

        // If this bucket belongs to our Morton key, accumulate data
        if (oldKey == mortonCode) {
            atomicAdd(&table[currentSlot].sumX, x[idx]);
            atomicAdd(&table[currentSlot].sumY, y[idx]);
            atomicAdd(&table[currentSlot].sumZ, z[idx]);

            atomicAdd(&table[currentSlot].sumR, (uint32_t)r[idx]);
            atomicAdd(&table[currentSlot].sumG, (uint32_t)g[idx]);
            atomicAdd(&table[currentSlot].sumB, (uint32_t)b[idx]);

            atomicAdd(&table[currentSlot].count, 1);
            return;
        }

        // Otherwise: collision with a different key, continue probing
    }
}
\end{lstlisting}

Note that this kernel does \emph{not} use \texttt{PointAccum} or \texttt{PointAccumOp}; all aggregation happens directly inside the \texttt{HashBucket}.

\paragraph{Counting valid voxels.}
Because open addressing leaves gaps in the hash table, a compaction pass is required. The first step is to count how many buckets are actually occupied:

\begin{lstlisting}[language=CUDA,caption={Kernel counting the number of occupied buckets (hash voxelizer).}]
__global__ void countValidBucketsKernel(
    HashBucket* table,
    size_t capacity,
    uint32_t* counter)
{
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < capacity) {
        if (table[idx].key != EMPTY_KEY) {
            atomicAdd(counter, 1);
        }
    }
}
\end{lstlisting}

\paragraph{Collecting results.}
A second pass converts the populated buckets into a dense array of voxel representatives. As in the sorting-based implementation, each voxel is represented by its centroid and average color:

\begin{lstlisting}[language=CUDA,caption={Kernel converting hash buckets into final voxel points (hash voxelizer).}]
__global__ void collectResultsKernel(
    HashBucket* table,
    size_t capacity,
    Point* output,
    uint32_t* globalCounter)
{
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= capacity) return;

    HashBucket bucket = table[idx];

    if (bucket.key != EMPTY_KEY && bucket.count > 0) {
        uint32_t outIdx = atomicAdd(globalCounter, 1);

        float c = (float)bucket.count;

        Point p;
        p.x = bucket.sumX / c;
        p.y = bucket.sumY / c;
        p.z = bucket.sumZ / c;
        p.r = (uint8_t)(bucket.sumR / bucket.count);
        p.g = (uint8_t)(bucket.sumG / bucket.count);
        p.b = (uint8_t)(bucket.sumB / bucket.count);

        output[outIdx] = p;
    }
}
\end{lstlisting}

\paragraph{Integration in the outer loop.}
The host-side driver for the dynamic hash map voxelizer can then be summarized as:

\begin{enumerate}
    \item \textbf{Capacity selection:} Choose a hash table capacity as a multiple of the number of points (e.g.\ factor 2--4).
    \item \textbf{Preprocessing:} Compute the bounding box and \texttt{inverse VoxelSize} on the CPU.
    \item \textbf{Device setup:} Allocate device arrays for point attributes and copy input data.
    \item \textbf{Hash table initialization:} Allocate and clear \texttt{HashBucket} array via \texttt{initHashMapKernel}.
    \item \textbf{Scatter-and-accumulate:} Launch \texttt{populateHashMapKernel}, which uses the shared \texttt{mortonEncode} to derive keys and accumulates directly into buckets.
    \item \textbf{Voxel counting:} Use \texttt{countValidBucketsKernel} to determine the number of occupied buckets and allocate a dense output array.
    \item \textbf{Compaction:} Reset the counter and call \texttt{collectResultsKernel} to write voxel centroids and colors into the dense output array.
    \item \textbf{Host transfer:} Copy the compact output back to the CPU and release GPU memory.
\end{enumerate}

In this way, the two voxelizers share a single, consistent Morton encoding implementation, while each maintains its own accumulation and reduction strategy tailored to its parallelization scheme.