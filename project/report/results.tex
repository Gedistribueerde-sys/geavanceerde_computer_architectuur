\section{Results and Analysis}

\subsection{Methodology and Experimental Setup}
All results were obtained on an NVIDIA GeForce GTX 1080 Ti (11GB GDDR5X) paired with an Intel Core i7-8700K CPU @ 4.8 GHz and 32GB DDR4 RAM, running Ubuntu 24.04.3 LTS. The test dataset consists of a dense point cloud scan of an urban environment containing 648,433 points.

To ensure robust results, the voxelizers were evaluated across a wide parameter space:
\begin{itemize}
    \item \textbf{Voxel Sizes:} 0.25, 0.5, 0.75, 1.0, 1.25.
    \item \textbf{Block Sizes:} 1, 2, 4, 8, 16, 32, 64, 256, 512, 1024 threads per block.
    \item \textbf{Hash-Table Capacity Factors:} 2, 3, and 4 times the input point count.
\end{itemize}
Every configuration was executed 100 times to obtain average execution times and minimize transient system load variations.

\subsection{Overall Performance: CPU vs. GPU}
The transition to GPU-based voxelization yields substantial performance improvements. As shown in Tables \ref{tab:cpu_min} and \ref{tab:cpu_max}, the GPU implementations achieve speedup factors ranging from \textbf{9.5$\times$ to 15.5$\times$} compared to the CPU.

The most significant gains occur at smaller voxel sizes (e.g., 0.25), where the massive parallelism of the GPU is fully exploited. Even in worst-case GPU scenarios, the speedup remains between 6.1$\times$ and 12.2$\times$.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Voxel Size} & \textbf{CPU (ms)} & \textbf{Morton (ms)} & \textbf{Hash CF2 (ms)} & \textbf{Hash CF3 (ms)} & \textbf{Hash CF4 (ms)} \\
\hline
0.25 & 429.09 & 36.61 & 27.70 & 29.57 & 29.10 \\
0.5 & 335.89 & 29.14 & 26.20 & 28.30 & 28.57 \\
0.75 & 272.39 & 27.11 & 25.89 & 26.57 & 27.74 \\
1.0 & 253.67 & 25.71 & 25.67 & 26.99 & 27.46 \\
1.25 & 240.00 & 25.18 & 25.85 & 27.24 & 28.28 \\
\hline
\end{tabular}
\caption{CPU vs Minimum GPU Voxelization Time}
\label{tab:cpu_min}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Voxel Size} & \textbf{CPU (ms)} & \textbf{Morton (ms)} & \textbf{Hash CF2 (ms)} & \textbf{Hash CF3 (ms)} & \textbf{Hash CF4 (ms)} \\
\hline
0.25 & 429.09 & 40.46 & 35.25 & 37.48 & 41.82 \\
0.5 & 335.89 & 31.18 & 33.87 & 35.53 & 40.00 \\
0.75 & 272.39 & 28.62 & 33.80 & 36.09 & 39.12 \\
1.0 & 253.67 & 27.63 & 33.36 & 35.26 & 38.53 \\
1.25 & 240.00 & 28.64 & 34.41 & 36.85 & 39.06 \\
\hline
\end{tabular}
\caption{CPU vs Maximum GPU Voxelization Time}
\label{tab:cpu_max}
\end{table}

\subsection{Detailed Analysis of GPU Methods}

\subsubsection{Morton-Code Voxelizer}
The Morton-based approach demonstrates consistent and predictable performance. It excels at larger voxel sizes (1.25 and above) where spatial coherence is high.
\begin{itemize}
    \item \textbf{Timing Breakdown:} The runtime is dominated by the \textbf{sorting stage} (0.90--1.02\,ms), which is constant across tests. Point accumulation takes 0.24--1.08\,ms.
    \item \textbf{Sensitivity:} Very small block sizes (1--4 threads) significantly slow down Morton-code generation (up to 1.07\,ms), whereas larger blocks stabilize this step to $\approx$0.04\,ms.
\end{itemize}

\subsubsection{Hash-Table Voxelizer}
The hash-based method is generally faster for small-to-medium voxel sizes but is highly sensitive to the \textbf{Capacity Factor (CF)}.
\begin{itemize}
    \item \textbf{CF 2 (Highest Efficiency):} Offers the best performance (25.67\,ms at voxel size 1.0) due to low memory overhead and fast device-to-host transfers ($\approx$2.71\,ms).
    \item \textbf{CF 3 \& 4 (Higher Overhead):} Increasing the table size reduces collisions but incurs significant penalties in initialization and memory transfer. CF 4 is 5--10\,ms slower than CF 2, with transfer times rising to 5.26\,ms.
\end{itemize}

\subsection{Parameter Sensitivity and Bottlenecks}

\subsubsection{Impact of Voxel Size}
Smaller voxel sizes generate a larger number of unique entries. This favors the \textbf{Hash-Table (CF 2)} method, which handles high fragmentation efficiently. Conversely, larger voxel sizes favor the \textbf{Morton} method, which benefits from predictable memory access patterns when spatial data is less fragmented.

\subsubsection{Impact of Block Size}
Both methods exhibit similar responses to thread block sizing:
\begin{itemize}
    \item \textbf{1--4 threads (Inefficient):} Severe performance penalties due to insufficient warp utilization.
    \item \textbf{8--256 threads (Optimal):} The "sweet spot" with minimal overhead.
    \item \textbf{512--1024 threads (Saturated):} No significant improvement; performance is likely limited by register pressure.
\end{itemize}

\subsection{Conclusions and Optimal Configurations}

Based on the analysis, the optimal strategy depends on the target voxel resolution:

\begin{enumerate}
    \item \textbf{Small Voxel Sizes (0.25--0.5):} Use the \textbf{Hash-based voxelizer (CF 2)} with a block size of 16--32 threads. It maximizes throughput by minimizing memory overhead.
    \item \textbf{Large Voxel Sizes (1.25+):} Use the \textbf{Morton-code voxelizer} with a block size of 64--256 threads. It leverages spatial coherence to outperform the hash method.
\end{enumerate}

\textbf{Limitations:} The Hash method is primarily bound by memory bandwidth (device-to-host transfer), while the Morton method is strictly bottlenecked by the sorting phase.

\subsection{Visualizing Global Performance}
\input{figures}